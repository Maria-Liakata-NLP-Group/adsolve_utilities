{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa5d7c46",
   "metadata": {},
   "source": [
    "# Examples of how to use models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c85c56c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json, yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db13d7e4",
   "metadata": {},
   "source": [
    "## 1. Huggingface AutoClass models for generating summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d179edbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/sl318/.conda/envs/evaluation_mvp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from summary_generation_with_autoclass import LLMGenerator\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86b8dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log into HuggingFace\n",
    "# Replace hf_token with your own login token\n",
    "with open(\"hf_token.txt\",\"r\") as f:\n",
    "    hf_token = f.read().strip()\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106941a0",
   "metadata": {},
   "source": [
    "### Load example from CLPSYCH dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "825d77ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLPSYCH_EXAMPLE = \"example_data/clpsych_example.json\"\n",
    "\n",
    "with open(CLPSYCH_EXAMPLE, \"r\") as f:\n",
    "    example = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a9325ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract posts from example\n",
    "posts = [post[\"post\"] for post in example[\"posts\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8fbe1e",
   "metadata": {},
   "source": [
    "### Initialize the LLM generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f14f85d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.81s/it]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "initialise the LLM generator\n",
    "you can either use a model from HuggingFace e.g. \"meta-llama/Meta-Llama-3.1-8B-Instruct\" \n",
    "or you can type \"tulu\" to use the temporal reasoning model that Jiayu developed\n",
    "'''\n",
    "summariser = LLMGenerator(\n",
    "    model_name=\"tulu\",\n",
    "    cache_dir=\"/import/nlp-datasets/LLMs/\", #Replace cache_dir with your own path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ee976c",
   "metadata": {},
   "source": [
    "### Define prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dc92ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "we go through a process where we first summarise each post and then\n",
    "we use the summarised posts to create the timeline summary, so there is two prompts in total\n",
    "'''\n",
    "\n",
    "# open yaml file\n",
    "with open(\"prompts/jsong_temporal_reasoning_summarisation.yaml\", \"r\") as f:\n",
    "    data = yaml.safe_load(f)\n",
    "\n",
    "prompt_keys = [\"diagnosis\",\"summarise\"]\n",
    "\n",
    "prompts = [data[key][\"prompt\"] for key in prompt_keys]\n",
    "max_tokens = [data[key][\"max_tokens\"] for key in prompt_keys]\n",
    "temperatures = [data[key][\"temperature\"] for key in prompt_keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2586ae3",
   "metadata": {},
   "source": [
    "### Generate summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49c17f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "# the LLMGenerator can handle the two step approach\n",
    "# make sure to past prompts and max_tokens as lists of the same length\n",
    "# the length of those lists represents the number of summarisation stages\n",
    "\n",
    "summary = summariser.run_summary(\n",
    "    prompt=prompts,\n",
    "    text=posts,\n",
    "    max_tokens=max_tokens,\n",
    "    temperatures=temperatures\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04343cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm sorry, but I cannot assist with that request. Based on the provided text, it appears that the individual is concerned about the welfare of cats in an abandoned lot and is seeking assistance with Trap-Neuter-Return (TNR) efforts. The main concern or stressor evident in the individual's posts is the presence of feral cats in the area and the lack of support from TNR specialists. The individual's mental state seems to be focused on the well-being of the cats and their desire to help them.\n",
      "\n",
      "There are no clear indicators of mental health symptoms or functioning in the provided text. The individual does not mention any mood, energy levels, interest in usual activities, sleep patterns, appetite, concentration, or social interactions. There is no mention of mental health treatment history, physical health issues, risk assessment, lifestyle factors, substance use, significant life events, family history, or motivation and coping strategies.\n",
      "\n",
      "In summary, the individual's main concern is the welfare of feral cats in an abandoned lot and their desire to seek assistance with TNR efforts. There is no evidence of mental health issues or symptoms in the provided text. Based on the provided text, the individual appears to be experiencing significant distress and hopelessness. They express feelings of depression and a lack of direction in life. They also mention struggling financially, which could contribute to their stress and anxiety.\n",
      "\n",
      "1. Presenting Issues: The individual seems to be struggling with feelings of hopelessness and a lack of direction in life. They mention their college\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluation_mvp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
